{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tensorflow","text":""},{"location":"#building-a-tensorflow-model","title":"Building a TensorFlow model","text":"<p>Here we have used <code>SGD</code> (Stochastic Gradient Descent) optimizer and <code>mae</code> (mean absolute error) loss function. We can also use <code>Adam</code> optimizer i.e. <code>tf.keras.optimize.Adam(learning_rate=0.001)</code> <pre><code>import tensorflow as tf\n# Set random seed to have reproducibility\ntf.random.set_seed(42)\n\n# 1. Create a model using the Sequential API\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, activation=\"relu\"),\n    #... other required layers\n])\n\n# 2. Compile the model\nmodel.compile(loss=tf.keras.losses.mae, # mae is mean absolute error\n              optimizer=tf.keras.optimizers.SGD(), # sgd is short for stochastic gradient descent\n              metrics=[\"mae\",\"accuracy\"])\n\n#3. Fit the model\nmodel.fit(X_tens_converted,y, epochs=5)\n</code></pre></p>"},{"location":"#getting-model-summery","title":"Getting model summery","text":"<p>We can get details about the model using <code>model.summary()</code> function. It will provide the list of total parameters and parametrs in each layer of the neural network.</p> <pre><code>model.summary()\n</code></pre>"},{"location":"#evaluating-the-model","title":"Evaluating the model","text":"<pre><code>model.evaluate(X_test,y_test)\n</code></pre>"},{"location":"#getting-a-prediction-from-a-model","title":"Getting a prediction from a model","text":"<pre><code>model.predict([6])\n</code></pre>"},{"location":"#saving-the-model","title":"Saving the model","text":""},{"location":"#savedmodel-format","title":"SavedModel format","text":"<p>The SavedModel format is a way to serialize models. Models saved in this format can be restored using <code>tf.keras.models.load_model</code> and are compatible with TensorFlow Serving. <pre><code># Save the entire model as a SavedModel.\n!mkdir -p saved_model\nmodel.save('saved_model/my_model')\n</code></pre></p>"},{"location":"#hdf5-format","title":"HDF5 format","text":"<p>Keras provides a basic legacy high-level save format using the HDF5 standard. <pre><code># Save the entire model to a HDF5 file.\n# The '.h5' extension indicates that the model should be saved to HDF5.\nmodel.save('my_model.h5')\n</code></pre></p>"},{"location":"#loading-the-model","title":"Loading the model","text":"<p>Model can be loaded from both SavedModel and HDF5 format <pre><code>#loading from SavedModel format\nfrom_saved_model = tf.keras.models.load_model('saved_model/my_model')\n#loading from HDF5 format\nfrom_h5 = tf.keras.models.load_model('my_model.h5')\n</code></pre></p>"},{"location":"#visualizing-the-model","title":"Visualizing the model","text":"<p>We can visualize the model using <code>plot_model</code></p> <pre><code>from tensorflow.keras.utils import plot_model\n\nplot_model(model=model,show_shapes=True)\n</code></pre>"},{"location":"#finding-the-error","title":"Finding the error","text":""},{"location":"#mean-absolute-error","title":"Mean Absolute Error","text":"<p>We can write following function which will give us the <code>mean absolute error</code>. <pre><code>def mae(y_true,y_pred):\n    return tf.metrics.mean_absolute_error(\n      y_true=y_test,\n      y_pred=tf.squeeze(y_pred)\n    )\n</code></pre></p>"},{"location":"#mean-squared-error","title":"Mean Squared Error","text":"<p>We can write following function which will give us <code>the mean squared error</code>. <pre><code>def mse(y_true,y_pred):\n    return tf.metrics.mean_squared_error(\n      y_true=y_test,\n      y_pred=tf.squeeze(y_pred)\n    )\n</code></pre></p>"},{"location":"pandas/","title":"Pandas","text":""},{"location":"pandas/#loading-a-csv-file","title":"Loading a csv file","text":"<pre><code>import pandas as pd\ndf = pd.read_csv(\"path/to/file.csv\")\n</code></pre>"},{"location":"pandas/#one-hot-encoding","title":"One Hot Encoding","text":"<pre><code>df_one_hot = pd.get_dummies(df)\n</code></pre>"},{"location":"pandas/#dropping-columns","title":"Dropping columns","text":"<pre><code>X = df.drop_column(\"column_name\", axis=1)\n</code></pre>"},{"location":"pandas/#getting-a-specific-column","title":"Getting a specific column","text":"<pre><code>y = df['column_name']\n</code></pre>"},{"location":"pandas/#plotting-the-loss-curve","title":"Plotting the loss curve","text":"<p>We can plot the loss curve in the following way: <pre><code>history = model.fit(.....)\n\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\n</code></pre></p>"},{"location":"pandas/#plotting-hist-plot","title":"Plotting hist plot","text":"<pre><code>X[['column1', 'column2', ...]].hist()\n</code></pre>"},{"location":"pandas/#decision-plot","title":"Decision Plot","text":"<pre><code>import numpy as np\n\ndef plot_decision_boundary(model, X, y):\n  \"\"\"\n  Plots the decision boundary created by a model predicting on X.\n  \"\"\"\n\n  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n\n  xx, yy = np.meshgrid(\n      np.linspace(x_min, x_max,100),\n      np.linspace(y_min, y_max,100))\n\n  # create X value (we're going to make predictions on these)\n  x_in  = np.c_[xx.ravel(), yy.ravel()] # stack 2D array together\n\n  # Make predictions\n  y_pred = model.predict(x_in)\n\n  # Check for multi-class\n  if (len(y_pred[0])&gt; 1):\n    print(\"doing multiclass classification\")\n    # we have to reshape our prediction to get them ready for plotting\n\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n  else:\n    print(\"doing binary classification\")\n    y_pred = np.round(y_pred).reshape(xx.shape)\n\n  # plot decision boundary\n  plt.contourf(xx,yy,y_pred,cmap=plt.cm.RdYlBu, alpha=0.7)\n  plt.scatter(X[:,0], X[:,1], c=y, s=40, cmap=plt.cm.RdYlBu)\n  plt.xlim(xx.min(), xx.max())\n  plt.ylim(yy.min(), yy.max())\n</code></pre>"},{"location":"sklearn/","title":"Scikit-Learn","text":""},{"location":"sklearn/#train-test-split","title":"Train-Test split","text":"<pre><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre>"},{"location":"sklearn/#normalization","title":"Normalization","text":"<pre><code>from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Create a column transformer\nct = make_column_transformer(\n    (MinMaxScaler(), [\"column1\", \"column2\", \"column3\"]), # turn all values in these columns between zero and ones\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"column1\", \"column2\", \"column3\"])\n)\n\n# Create X &amp; y values\nX = df.drop(\"charges\", axis=1)\ny = df[\"charges\"]\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the column transformer to our training data\nct.fit(X_train)\n\n# transform training and test data with normalization (MinMaxScaler and OneHotEncoding)\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)\n</code></pre>"},{"location":"sklearn/#creating-random-samples","title":"Creating Random Samples","text":"<pre><code>from sklearn.datasets import make_circles\n\n# Make 1000 examples\nn_samples = 1000\n\n# Create circles\nX,y = make_circles(n_samples, noise=0.03, random_state=42)\n</code></pre>"}]}